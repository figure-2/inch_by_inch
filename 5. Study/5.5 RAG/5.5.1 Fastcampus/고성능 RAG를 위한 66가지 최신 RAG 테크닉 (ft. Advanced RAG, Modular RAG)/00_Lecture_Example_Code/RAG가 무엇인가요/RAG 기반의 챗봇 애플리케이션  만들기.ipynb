{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfd5db-6aa2-4649-b919-cc0c3c66bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit\n",
    "import streamlit as st\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from decouple import config\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"  #openai 키 입력\n",
    "\n",
    "# Prompt 템플릿 정의 \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"You are a AI assistant. You are\n",
    "    currently having a conversation with a human. Answer the questions.\n",
    "    \n",
    "    chat_history: {chat_history},\n",
    "    Human: {question}\n",
    "    AI:\"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,  # 창의성 0으로 설정 \n",
    "                 model_name='gpt-4-0314',  # 모델명\n",
    "                )\n",
    "#윈도우 크기 k를 지정하면 최근 k개의 대화만 기억하고 이전 대화는 삭제\n",
    "# 멀티턴을 구현 ConversationBufferWindowMemory을 사용해서  k=4 의 질문과 답변 대화를 기억 하게 함\n",
    "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=4) \n",
    "\n",
    "# LLM Chain 정의\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# st는 streamlit이며, 애플리케이션으로 쉽게 구현할수 있게 도와주는 라이브러리\n",
    "st.title(\"ChatGPT AI Assistant\")\n",
    "\n",
    "# 세션에서 메시지를 확인하고 존재하지 않는 경우 생성\n",
    "# 대화를 한적이 있는이 없는지를 판단/ 대화가 없으면 아래 문구를 출력\n",
    "if \"messages\" not in st.session_state.keys():\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"assistant\", \"content\": \"안녕하세요, 저는 AI Assistant입니다.\"}\n",
    "    ]\n",
    "\n",
    "# 모든 메시지 표시\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "\n",
    "user_prompt = st.chat_input()\n",
    "\n",
    "# 사용자 입력 보여주기\n",
    "if user_prompt is not None:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_prompt}) # 내가 계속해서 질문하고 답변 내용을 append 해주는 부분\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(user_prompt)\n",
    "\n",
    "# 마지막 메시지가 assistant로 부터 받은게 아니라면 새로운 답변 생성\n",
    "# messages[-1] 이것이 대화 종료한 마지막 부분이 내가 질문한것인지 아니면 질문에 대한 답변인지 대화의 마지막 부분이 뭔지 확인하는것\n",
    "# 만약 대화 마지막이 LLM이 답변한것이 아니라면 질문에 대해 답변을 하게 해주는것\n",
    "\n",
    "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Loading...\"):\n",
    "            ai_response = llm_chain.predict(question=user_prompt)\n",
    "            st.write(ai_response)\n",
    "    new_ai_message = {\"role\": \"assistant\", \"content\": ai_response}\n",
    "    st.session_state.messages.append(new_ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fc59a-8a6b-4d10-b8f5-8ecc0455046e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_f",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
