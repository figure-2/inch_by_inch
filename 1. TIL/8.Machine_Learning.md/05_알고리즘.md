# 알고리즘
> 회귀_선형회귀(Linear Regression)  
> 결정트리(DecisionTree)  
> K최근접이웃모델(K-Nearest Neighbors : KNN) 
> 분류_로지스틱 회귀(Logistic Regression)   

## 회귀
- 머신러닝 회귀 예측의 핵심은 주어진 피처와 레이블을 기반으로 학습을 통해 최적의 회귀 계수를 찾아내는 것 (회귀계수 W1,W2,...,Wn을 찾는 것).

- 선형 회귀 : 성능 아쉬움, 1차원
- 다항 회귀 : 2/3차 방정식과 같은 다항식을 쓰는 모델로 회귀선이 곡선임.

- 모델 적합 과정 (훈련을 한다는 것)
1. TrainData를 model에 input
2. 모델 learning
3. predict
4. 예측과 정답 비교해서 손실함수(RSS)가 최소??
    - 최소가 아니다.
        - 가중치와 절편 업데이트해서 input
        - learning
        - predict
        - 손실함수 최소? 반복
    - 최소이다.
        - 학습종료.

*가중치(weight)와 절편(bias using optimizer)을 업데이트함으로써 손실함수를 최소로 만들고 힉습을 종료함.*  

*오차가 많은데 이건 맞는 모델이고 정확도가 100%인 모델을 만들 수 없고 100%가 되었더라도 과적합되었구나라고 생각됨.*
```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# 데이터 로드

x_data = np.array([
    [1, 1],
    [2, 2],
    [3, 4],
    [4, 5],
    [5, 5],
    [6, 5],
    [7, 9],
    [8, 10],
    [9, 12],
    [10, 2],
    [11, 10],
    [12, 4]
])
y_data = np.array([3, 5, 7, 10, 12, 7, 13, 13, 12, 13, 12, 6]) # 레이블

# 데이터 분석
pass

# 데이터 전처리
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=777) 
# test_size=0.3 학습용(7) : 테스트용 (3) 으로 분할 
# random_state=777 동일한 학습/테스트용 데이터 세트를 생성 위해


# 모델 생성
model = LinearRegression()

# 모델 학습
model.fit(x_train, y_train)

# 모델 검증
pass

# 모델 예측
x_test = np.array([
    [4, 6]
])

y_predict = model.predict(x_test)

print(y_predict[0]) #9.4827304868099
```
## 결정트리
- 이론
    - 스무고개와 같이 특정 질문들을 통해 정답을 찾아가는 모델
    - 최상단의 뿌리마디에서 마지막 끝 마디까지 아래 방향으로 성장
    - 최선의 분할은 데이터가 균일해지도록(순도가 높아짐, 불순도가 낮아짐) 분리기준을 잡습니다.
    - 불순도값에 의해 분리됨.
- 분리기준
    - 불순도 : 다른 데이터가 섞여 있는 정도
    - 지니불순도가 가장 낮은값을 갖는 기준으로 결정트리를 성장시킴.
    - 정보이득이 높은 값을 갖는 기준으로 결정트리를 성장시킴.
    - 즉, 불순한 것이 사라지도록 또는 정보가 점점 순수해져 분명하게 얻는 정보가 많아지도록 분리 기준을 잡습니다. 
- 장점
    - 해석이 직관적이고 쉽다.
    - 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않습니다.
- 단점
    - 과적합으로 알고리즘 성능이 떨어짐
    - 이를 극복하기 위해서 트리의 크기나 리프노드의 샘플 수 등 파라미터들을 제한하는 튜닝이 필요함.
```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree

import warnings
warnings.filterwarnings('ignore')

# 데이터 로드
iris_data = load_iris()

# 데이터 분석
# 데이터 전처리
x_train , x_test , y_train , y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2,  random_state=11)

# 모델 생성
1. model = DecisionTreeClassifier(random_state=156,max_depth=3) # 트리성장 깊이 조절 max_depth : 하이퍼파라미터 -> 머신러닝, 딥러닝에서 학스을 할 때 개발자가 학습 정도를 조절하는데 사용하는 파라미터
2. model = DecisionTreeClassifier(random_state=156,min_samples_leaf=5) # 리프노드의 최소 샘플 수 (리프노드가 5개일때까지 성장하라 => value = [0,3,5], [0,0,5])
3. model = DecisionTreeClassifier(random_state=111,min_samples_leaf=2,max_depth=4) # 리프노드 & 트리성장 깊이 조절 중 어느것이 우선?

# 하이퍼파라미터 중에는 기능적으로 상충 및 중복기능이 있을 수 있는데, 그 중 우선권을 가지는 파라미터가 있다.
# 여러개의 하이퍼파라미터를 사용할 때, 중복의미를 가지지는 않는지 확인 하자

# 모델 학습
model.fit(x_train , y_train)

# 모델 검증
print(model.score(x_train, y_train))
print(model.score(x_test, y_test)) 


# 모델 예측
plt.figure(figsize=(15,10))
plot_tree(model,filled=True, feature_names=iris_data.feature_names)

plt.show()
```

## K-Nearest Neighbors : K최근접 이웃 모델
- 분류하고자 하는 특성과 가장 가까운 k개(default 5개)를 찾음
- 그 중에 가장 많은 범주로 예측(다수결 투표)
- K(주변의갯수)라는 값 조절 적잘한 모델 찾음

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# 데이터 로드
x_data = np.array([
    [2, 1],
    [3, 2],
    [3, 4],
    [5, 5],
    [7, 5],
    [2, 5],
    [8, 9],
    [9, 10],
    [6, 12],
    [9, 2],
    [6, 10],
    [2, 4]
])
y_data = np.array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])

labels = ['fail', 'pass']

# 데이터 분석
pass

# 데이터 전처리
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=777, stratify=y_data)

# 모델 생성
model = KNeighborsClassifier(n_neighbors=7) #하이퍼파라미터 튜닝 => 파라미터 모를 때 : 빈 셀에 KNeighborsClassifier?

# 모델 학습
result = model.fit(x_train, y_train)

# 모델 검증
print(model.score(x_train, y_train)) #

print(model.score(x_test, y_test)) #0.75

# 모델 예측
x_test = np.array([
    [4, 6]
])

y_predict = model.predict(x_test)
label = labels[y_predict[0]]
y_predict = model.predict_proba(x_test)
confidence = y_predict[0][y_predict[0].argmax()]

print(label, confidence)
```

## 로지스틱 회귀
- 로지스틱 회귀는 **분류에 사용됨**. 회귀 아님!
- 시그모이드 함수(Sigmoid Function)을 사용해서, 연속적인 값을 0과 1 사이의 값으로 변환해 줍니다. 
- 로지스틱 회귀는 정확히 0 또는 1을 예측하는 대신, 확률(0과1사이의 값)을 생성해서 0과 1을 분류하는 예측 모델
- 로지스틱 회귀에서 예측은 **예측 확률**을 의미합니다. 
- 예측 확률이 0.5 이상이면 1로, 0.5 이하이면 0으로 예측합니다. 

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# 데이터 로드

## [낮잠시간, 공부시간]
x_data = np.array([
    [2, 1],
    [3, 2],
    [3, 4],
    [5, 5],
    [7, 5],
    [2, 5],
    [8, 9],
    [9, 10],
    [6, 12],
    [9, 2],
    [6, 10],
    [2, 4]
])
y_data = np.array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])

labels = ['fail', 'pass']

# 데이터 분석

# 데이터 전처리

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=777, stratify=y_data)

# 모델 생성

model = LogisticRegression()

# 모델 학습

model.fit(x_train, y_train)

# 모델 검증
평가척도 배운 후 진행. (아직안배움)

# 모델 예측

x_real = np.array([
    [4, 6]
])

y_real_pred = model.predict(x_real)

print(y_real_pred) #[1] => PASS 배열로 나옴
print(y_real_pred[0]) #1  => PASS 배열에서 값 빼려고
label = labels[y_real_pred[0]] # 예측값으로 label 글자 출력
label

y_real_proba = model.predict_proba(x_real) # 예측 확률값
confidence = y_real_proba[0][y_real_proba[0].argmax()] 

print(label, confidence) # pass 0.7439902354649888
print(y_real_proba) # [[0.25600976 0.74399024]]

# 예측 결과값과 예측 확률값 출력
```