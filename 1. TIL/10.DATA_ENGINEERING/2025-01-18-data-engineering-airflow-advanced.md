---
title: Apache Airflow ìƒì„¸ ë‚´ìš©
categories:
- 1.TIL
- 1-9.DATA_ENGINEERING
tags:
- ë°ì´í„°ì—”ì§€ë‹ˆì–´ë§
- Airflow
- ì›Œí¬í”Œë¡œìš°
- ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- DAG
- ETL
- ë°ì´í„°í’ˆì§ˆê²€ì‚¬
- ë¨¸ì‹ ëŸ¬ë‹íŒŒì´í”„ë¼ì¸
- ëª¨ë‹ˆí„°ë§
- ì„±ëŠ¥ìµœì í™”
- ë³´ì•ˆ
toc: true
date: 2023-11-09 10:00:00 +0900
comments: false
mermaid: true
math: true
---
# Airflow ìƒì„¸ ë‚´ìš©

> 231120~231121, 231215~231222 í•™ìŠµí•œ ë‚´ìš© ì •ë¦¬

## Apache Airflow ê°œìš”

### ì •ì˜
- **Apache Airflow**: ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í”Œë«í¼
- **DAG**: Directed Acyclic Graph (ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„)
- **ìŠ¤ì¼€ì¤„ë§**: ë³µì¡í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìŠ¤ì¼€ì¤„ë§ ë° ëª¨ë‹ˆí„°ë§
- **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: ì‘ì—… ê°„ ì˜ì¡´ì„± ê´€ë¦¬ ë° ì‹¤í–‰ ìˆœì„œ ì œì–´

### íŠ¹ì§•
- **ì‹œê°í™”**: ì›¹ UIë¥¼ í†µí•œ ì›Œí¬í”Œë¡œìš° ì‹œê°í™”
- **ìŠ¤ì¼€ì¤„ë§**: Cron ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§
- **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì‘ì—… ëª¨ë‹ˆí„°ë§
- **í™•ì¥ì„±**: ë‹¤ì–‘í•œ ì—°ì‚°ìì™€ í”ŒëŸ¬ê·¸ì¸ ì§€ì›

### ì¥ì 
- **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ì‘ì—… ìœ í˜• ì§€ì›
- **ëª¨ë‹ˆí„°ë§**: ìƒì„¸í•œ ì‹¤í–‰ ë¡œê·¸ ë° ëª¨ë‹ˆí„°ë§
- **ì¬ì‚¬ìš©ì„±**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸
- **í™•ì¥ì„±**: ì»¤ìŠ¤í…€ ì—°ì‚°ì ë° í”ŒëŸ¬ê·¸ì¸ ê°œë°œ

## Airflow ì„¤ì¹˜ ë° ì„¤ì •

### 1. Airflow ì„¤ì¹˜
```bash
# pip ì„¤ì¹˜
pip install apache-airflow

# íŠ¹ì • ë²„ì „ ì„¤ì¹˜
pip install apache-airflow==2.5.0

# ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install apache-airflow[postgres,celery,redis]
```

### 2. Airflow ì´ˆê¸°í™”
```bash
# Airflow í™ˆ ë””ë ‰í† ë¦¬ ì„¤ì •
export AIRFLOW_HOME=~/airflow

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
airflow db init

# ê´€ë¦¬ì ì‚¬ìš©ì ìƒì„±
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

### 3. Airflow ì‹œì‘
```bash
# ì›¹ ì„œë²„ ì‹œì‘
airflow webserver --port 8080

# ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
airflow scheduler
```

## Airflow ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. DAG ìƒì„±
```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator

# ê¸°ë³¸ ì¸ìˆ˜ ì„¤ì •
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# DAG ì •ì˜
dag = DAG(
    'example_dag',
    default_args=default_args,
    description='ì˜ˆì œ DAG',
    schedule_interval=timedelta(days=1),
    catchup=False,
    tags=['example', 'tutorial'],
)

# ì‘ì—… ì •ì˜
start_task = DummyOperator(
    task_id='start',
    dag=dag,
)

# Bash ì‘ì—…
bash_task = BashOperator(
    task_id='bash_task',
    bash_command='echo "Hello Airflow!"',
    dag=dag,
)

# Python ì‘ì—…
def python_function():
    print("Python ì‘ì—… ì‹¤í–‰")
    return "ì‘ì—… ì™„ë£Œ"

python_task = PythonOperator(
    task_id='python_task',
    python_callable=python_function,
    dag=dag,
)

# ì‘ì—… ì˜ì¡´ì„± ì„¤ì •
start_task >> bash_task >> python_task
```

### 2. Python ì‘ì—…
```python
from airflow.operators.python import PythonOperator
from airflow.operators.python import BranchPythonOperator
from airflow.operators.python import ShortCircuitOperator

# ê¸°ë³¸ Python ì‘ì—…
def data_processing():
    """ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜"""
    import pandas as pd
    import numpy as np
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    data = {
        'id': range(1, 101),
        'value': np.random.randn(100),
        'category': np.random.choice(['A', 'B', 'C'], 100)
    }
    
    df = pd.DataFrame(data)
    
    # ë°ì´í„° ì²˜ë¦¬
    processed_df = df.groupby('category').agg({
        'value': ['mean', 'std', 'count']
    }).round(2)
    
    print("ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ:")
    print(processed_df)
    
    return processed_df.to_dict()

python_task = PythonOperator(
    task_id='data_processing',
    python_callable=data_processing,
    dag=dag,
)

# ë¶„ê¸° ì‘ì—…
def decide_branch():
    """ë¶„ê¸° ê²°ì • í•¨ìˆ˜"""
    import random
    
    if random.random() > 0.5:
        return 'branch_a'
    else:
        return 'branch_b'

branch_task = BranchPythonOperator(
    task_id='branch_task',
    python_callable=decide_branch,
    dag=dag,
)

# ë¶„ê¸° ì‘ì—…ë“¤
branch_a_task = DummyOperator(
    task_id='branch_a',
    dag=dag,
)

branch_b_task = DummyOperator(
    task_id='branch_b',
    dag=dag,
)

# ë‹¨ì¶• íšŒë¡œ ì‘ì—…
def should_continue():
    """ê³„ì† ì§„í–‰í• ì§€ ê²°ì •"""
    import random
    return random.random() > 0.3

short_circuit_task = ShortCircuitOperator(
    task_id='short_circuit_task',
    python_callable=should_continue,
    dag=dag,
)

# ì‘ì—… ì˜ì¡´ì„±
python_task >> branch_task
branch_task >> [branch_a_task, branch_b_task]
branch_a_task >> short_circuit_task
branch_b_task >> short_circuit_task
```

### 3. ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—…
```python
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# PostgreSQL ì‘ì—…
postgres_task = PostgresOperator(
    task_id='postgres_task',
    postgres_conn_id='postgres_default',
    sql='''
    CREATE TABLE IF NOT EXISTS test_table (
        id SERIAL PRIMARY KEY,
        name VARCHAR(100),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    ''',
    dag=dag,
)

# PostgreSQL Hook ì‚¬ìš©
def postgres_hook_example():
    """PostgreSQL Hook ì‚¬ìš© ì˜ˆì‹œ"""
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # ë°ì´í„° ì‚½ì…
    insert_sql = """
    INSERT INTO test_table (name) VALUES (%s)
    """
    
    hook.run(insert_sql, parameters=['test_name'])
    
    # ë°ì´í„° ì¡°íšŒ
    select_sql = "SELECT * FROM test_table LIMIT 10"
    records = hook.get_records(select_sql)
    
    print("ì¡°íšŒëœ ë°ì´í„°:")
    for record in records:
        print(record)

postgres_hook_task = PythonOperator(
    task_id='postgres_hook_task',
    python_callable=postgres_hook_example,
    dag=dag,
)
```

## Airflow ê³ ê¸‰ ê¸°ëŠ¥

### 1. XCom (Cross-Communication)
```python
from airflow.operators.python import PythonOperator

# XComì„ ì‚¬ìš©í•œ ë°ì´í„° ì „ë‹¬
def push_data():
    """ë°ì´í„°ë¥¼ XComì— í‘¸ì‹œ"""
    return {
        'processed_data': [1, 2, 3, 4, 5],
        'metadata': {
            'processed_at': datetime.now().isoformat(),
            'record_count': 5
        }
    }

def pull_data(**context):
    """XComì—ì„œ ë°ì´í„°ë¥¼ í’€"""
    # ì´ì „ ì‘ì—…ì˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    data = context['task_instance'].xcom_pull(task_ids='push_data_task')
    
    print("ë°›ì€ ë°ì´í„°:")
    print(f"ì²˜ë¦¬ëœ ë°ì´í„°: {data['processed_data']}")
    print(f"ë©”íƒ€ë°ì´í„°: {data['metadata']}")
    
    # ë°ì´í„° ì²˜ë¦¬
    processed_data = [x * 2 for x in data['processed_data']]
    
    return processed_data

def final_processing(**context):
    """ìµœì¢… ì²˜ë¦¬"""
    data = context['task_instance'].xcom_pull(task_ids='pull_data_task')
    
    print(f"ìµœì¢… ì²˜ë¦¬ëœ ë°ì´í„°: {data}")
    
    return sum(data)

# XCom ì‘ì—…ë“¤
push_data_task = PythonOperator(
    task_id='push_data_task',
    python_callable=push_data,
    dag=dag,
)

pull_data_task = PythonOperator(
    task_id='pull_data_task',
    python_callable=pull_data,
    dag=dag,
)

final_processing_task = PythonOperator(
    task_id='final_processing_task',
    python_callable=final_processing,
    dag=dag,
)

# ì‘ì—… ì˜ì¡´ì„±
push_data_task >> pull_data_task >> final_processing_task
```

### 2. ë™ì  DAG ìƒì„±
```python
from airflow import DAG
from airflow.operators.bash import BashOperator

# ë™ì  DAG ìƒì„± í•¨ìˆ˜
def create_dynamic_dag(dag_id, schedule_interval, tasks):
    """ë™ì  DAG ìƒì„±"""
    
    default_args = {
        'owner': 'data_team',
        'start_date': datetime(2023, 1, 1),
        'retries': 1,
        'retry_delay': timedelta(minutes=5),
    }
    
    dag = DAG(
        dag_id,
        default_args=default_args,
        description=f'ë™ì  ìƒì„±ëœ DAG: {dag_id}',
        schedule_interval=schedule_interval,
        catchup=False,
    )
    
    # ë™ì ìœ¼ë¡œ ì‘ì—… ìƒì„±
    previous_task = None
    
    for i, task_config in enumerate(tasks):
        task = BashOperator(
            task_id=f'task_{i}',
            bash_command=task_config['command'],
            dag=dag,
        )
        
        if previous_task:
            previous_task >> task
        
        previous_task = task
    
    return dag

# ë™ì  DAG ìƒì„± ì˜ˆì‹œ
tasks_config = [
    {'command': 'echo "ì‘ì—… 1 ì‹¤í–‰"'},
    {'command': 'echo "ì‘ì—… 2 ì‹¤í–‰"'},
    {'command': 'echo "ì‘ì—… 3 ì‹¤í–‰"'},
]

dynamic_dag = create_dynamic_dag(
    'dynamic_example_dag',
    timedelta(hours=1),
    tasks_config
)
```

### 3. ì»¤ìŠ¤í…€ ì—°ì‚°ì
```python
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults

class CustomOperator(BaseOperator):
    """ì»¤ìŠ¤í…€ ì—°ì‚°ì"""
    
    @apply_defaults
    def __init__(self, custom_param, *args, **kwargs):
        super(CustomOperator, self).__init__(*args, **kwargs)
        self.custom_param = custom_param
    
    def execute(self, context):
        """ì—°ì‚°ì ì‹¤í–‰"""
        print(f"ì»¤ìŠ¤í…€ ì—°ì‚°ì ì‹¤í–‰: {self.custom_param}")
        
        # ì»¤ìŠ¤í…€ ë¡œì§ êµ¬í˜„
        result = self.process_data()
        
        return result
    
    def process_data(self):
        """ë°ì´í„° ì²˜ë¦¬ ë¡œì§"""
        # ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ë¡œì§
        processed_data = {
            'input': self.custom_param,
            'output': f"processed_{self.custom_param}",
            'timestamp': datetime.now().isoformat()
        }
        
        return processed_data

# ì»¤ìŠ¤í…€ ì—°ì‚°ì ì‚¬ìš©
custom_task = CustomOperator(
    task_id='custom_task',
    custom_param='test_value',
    dag=dag,
)
```

## Airflow ì‹¤ë¬´ ì ìš© ì˜ˆì‹œ

### 1. ETL íŒŒì´í”„ë¼ì¸
```python
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator

# ETL íŒŒì´í”„ë¼ì¸ DAG
etl_dag = DAG(
    'etl_pipeline',
    default_args=default_args,
    description='ETL íŒŒì´í”„ë¼ì¸',
    schedule_interval=timedelta(hours=1),
    catchup=False,
)

# Extract ì‘ì—…
def extract_data():
    """ë°ì´í„° ì¶”ì¶œ"""
    import requests
    import json
    
    # APIì—ì„œ ë°ì´í„° ì¶”ì¶œ
    response = requests.get('https://api.example.com/data')
    data = response.json()
    
    # ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    with open('/tmp/extracted_data.json', 'w') as f:
        json.dump(data, f)
    
    print(f"ì¶”ì¶œëœ ë°ì´í„° ìˆ˜: {len(data)}")
    return len(data)

extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=etl_dag,
)

# Transform ì‘ì—…
def transform_data():
    """ë°ì´í„° ë³€í™˜"""
    import json
    import pandas as pd
    
    # ë°ì´í„° ë¡œë“œ
    with open('/tmp/extracted_data.json', 'r') as f:
        data = json.load(f)
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(data)
    
    # ë°ì´í„° ë³€í™˜
    df['processed_at'] = datetime.now()
    df['value_squared'] = df['value'] ** 2
    
    # ë³€í™˜ëœ ë°ì´í„° ì €ì¥
    df.to_csv('/tmp/transformed_data.csv', index=False)
    
    print(f"ë³€í™˜ëœ ë°ì´í„° ìˆ˜: {len(df)}")
    return len(df)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=etl_dag,
)

# Load ì‘ì—…
def load_data():
    """ë°ì´í„° ë¡œë“œ"""
    import pandas as pd
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('/tmp/transformed_data.csv')
    
    # PostgreSQLì— ë°ì´í„° ì‚½ì…
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # í…Œì´ë¸” ìƒì„±
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS processed_data (
        id SERIAL PRIMARY KEY,
        value FLOAT,
        processed_at TIMESTAMP,
        value_squared FLOAT
    );
    """
    
    hook.run(create_table_sql)
    
    # ë°ì´í„° ì‚½ì…
    for _, row in df.iterrows():
        insert_sql = """
        INSERT INTO processed_data (value, processed_at, value_squared)
        VALUES (%s, %s, %s)
        """
        
        hook.run(insert_sql, parameters=[
            row['value'],
            row['processed_at'],
            row['value_squared']
        ])
    
    print(f"ë¡œë“œëœ ë°ì´í„° ìˆ˜: {len(df)}")
    return len(df)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=etl_dag,
)

# ETL íŒŒì´í”„ë¼ì¸ ì˜ì¡´ì„±
extract_task >> transform_task >> load_task
```

### 2. ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
```python
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator

# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ DAG
quality_dag = DAG(
    'data_quality_check',
    default_args=default_args,
    description='ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

# ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
def data_quality_check():
    """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    import pandas as pd
    
    hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # ë°ì´í„° ì¡°íšŒ
    query = "SELECT * FROM processed_data"
    df = pd.read_sql(query, hook.get_conn())
    
    # í’ˆì§ˆ ê²€ì‚¬
    quality_report = {
        'total_records': len(df),
        'null_values': df.isnull().sum().to_dict(),
        'duplicate_records': df.duplicated().sum(),
        'data_types': df.dtypes.to_dict(),
        'statistics': df.describe().to_dict()
    }
    
    # í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ ì €ì¥
    with open('/tmp/quality_report.json', 'w') as f:
        json.dump(quality_report, f, indent=2, default=str)
    
    print("ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì™„ë£Œ:")
    print(f"ì´ ë ˆì½”ë“œ ìˆ˜: {quality_report['total_records']}")
    print(f"ì¤‘ë³µ ë ˆì½”ë“œ ìˆ˜: {quality_report['duplicate_records']}")
    
    return quality_report

quality_check_task = PythonOperator(
    task_id='data_quality_check',
    python_callable=data_quality_check,
    dag=quality_dag,
)

# í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ ì•Œë¦¼
def send_quality_report(**context):
    """í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ ì•Œë¦¼"""
    import json
    
    # í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    quality_report = context['task_instance'].xcom_pull(task_ids='data_quality_check')
    
    # ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±
    message = f"""
    ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼:
    
    ì´ ë ˆì½”ë“œ ìˆ˜: {quality_report['total_records']}
    ì¤‘ë³µ ë ˆì½”ë“œ ìˆ˜: {quality_report['duplicate_records']}
    
    ìƒì„¸ ê²°ê³¼ëŠ” ì²¨ë¶€ íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.
    """
    
    # ì´ë©”ì¼ ì „ì†¡
    email_task = EmailOperator(
        task_id='send_quality_report',
        to=['data_team@example.com'],
        subject='ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼',
        html_content=message,
        files=['/tmp/quality_report.json'],
        dag=quality_dag,
    )
    
    return email_task

# í’ˆì§ˆ ê²€ì‚¬ íŒŒì´í”„ë¼ì¸ ì˜ì¡´ì„±
quality_check_task >> send_quality_report()
```

### 3. ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸
```python
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

# ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ DAG
ml_dag = DAG(
    'ml_pipeline',
    default_args=default_args,
    description='ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸',
    schedule_interval=timedelta(days=1),
    catchup=False,
)

# ë°ì´í„° ì „ì²˜ë¦¬
def data_preprocessing():
    """ë°ì´í„° ì „ì²˜ë¦¬"""
    import pandas as pd
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv('/tmp/raw_data.csv')
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    X = df.drop('target', axis=1)
    y = df['target']
    
    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42
    )
    
    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
    pd.DataFrame(X_train).to_csv('/tmp/X_train.csv', index=False)
    pd.DataFrame(X_test).to_csv('/tmp/X_test.csv', index=False)
    pd.DataFrame(y_train).to_csv('/tmp/y_train.csv', index=False)
    pd.DataFrame(y_test).to_csv('/tmp/y_test.csv', index=False)
    
    print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
    return "preprocessing_complete"

preprocessing_task = PythonOperator(
    task_id='data_preprocessing',
    python_callable=data_preprocessing,
    dag=ml_dag,
)

# ëª¨ë¸ í›ˆë ¨
def model_training():
    """ëª¨ë¸ í›ˆë ¨"""
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score
    import joblib
    
    # ë°ì´í„° ë¡œë“œ
    X_train = pd.read_csv('/tmp/X_train.csv')
    y_train = pd.read_csv('/tmp/y_train.csv')
    X_test = pd.read_csv('/tmp/X_test.csv')
    y_test = pd.read_csv('/tmp/y_test.csv')
    
    # ëª¨ë¸ í›ˆë ¨
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # ëª¨ë¸ í‰ê°€
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    # ëª¨ë¸ ì €ì¥
    joblib.dump(model, '/tmp/trained_model.pkl')
    
    print(f"ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ, ì •í™•ë„: {accuracy:.4f}")
    return accuracy

training_task = PythonOperator(
    task_id='model_training',
    python_callable=model_training,
    dag=ml_dag,
)

# ëª¨ë¸ ë°°í¬
def model_deployment():
    """ëª¨ë¸ ë°°í¬"""
    import shutil
    import os
    
    # ëª¨ë¸ íŒŒì¼ ë³µì‚¬
    source = '/tmp/trained_model.pkl'
    destination = '/opt/models/production_model.pkl'
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(destination), exist_ok=True)
    
    # ëª¨ë¸ íŒŒì¼ ë³µì‚¬
    shutil.copy2(source, destination)
    
    print("ëª¨ë¸ ë°°í¬ ì™„ë£Œ")
    return "deployment_complete"

deployment_task = PythonOperator(
    task_id='model_deployment',
    python_callable=model_deployment,
    dag=ml_dag,
)

# ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ ì˜ì¡´ì„±
preprocessing_task >> training_task >> deployment_task
```

## Airflow ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

### 1. ì•Œë¦¼ ì„¤ì •
```python
from airflow.operators.email import EmailOperator
from airflow.operators.slack import SlackAPIPostOperator

# ì‹¤íŒ¨ ì‹œ ì•Œë¦¼
def failure_callback(context):
    """ì‹¤íŒ¨ ì‹œ ì½œë°±"""
    task_instance = context['task_instance']
    
    # ì´ë©”ì¼ ì•Œë¦¼
    email_task = EmailOperator(
        task_id='failure_email',
        to=['admin@example.com'],
        subject=f'Airflow ì‘ì—… ì‹¤íŒ¨: {task_instance.task_id}',
        html_content=f"""
        ì‘ì—… ì‹¤íŒ¨ ì •ë³´:
        - DAG: {context['dag'].dag_id}
        - ì‘ì—…: {task_instance.task_id}
        - ì‹¤í–‰ ë‚ ì§œ: {context['ds']}
        - ì˜¤ë¥˜: {context['exception']}
        """,
    )
    
    # Slack ì•Œë¦¼
    slack_task = SlackAPIPostOperator(
        task_id='failure_slack',
        channel='#alerts',
        text=f'ğŸš¨ Airflow ì‘ì—… ì‹¤íŒ¨: {task_instance.task_id}',
        username='airflow-bot',
    )
    
    return [email_task, slack_task]

# DAGì— ì‹¤íŒ¨ ì½œë°± ì„¤ì •
dag = DAG(
    'monitored_dag',
    default_args=default_args,
    description='ëª¨ë‹ˆí„°ë§ë˜ëŠ” DAG',
    schedule_interval=timedelta(hours=1),
    catchup=False,
    on_failure_callback=failure_callback,
)
```

### 2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
from airflow.operators.python import PythonOperator
from airflow.models import TaskInstance

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
def performance_monitoring(**context):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    task_instance = context['task_instance']
    
    # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
    start_time = task_instance.start_date
    end_time = task_instance.end_date
    
    if start_time and end_time:
        execution_time = (end_time - start_time).total_seconds()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
        metrics = {
            'task_id': task_instance.task_id,
            'execution_time': execution_time,
            'start_time': start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'status': task_instance.state
        }
        
        # ë©”íŠ¸ë¦­ì„ íŒŒì¼ë¡œ ì €ì¥
        with open(f'/tmp/metrics_{task_instance.task_id}.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        
        print(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­: {metrics}")
    
    return metrics

performance_task = PythonOperator(
    task_id='performance_monitoring',
    python_callable=performance_monitoring,
    dag=dag,
)
```

### 3. ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
```python
import psutil
import os

# ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
def resource_monitoring():
    """ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
    
    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    # Airflow í”„ë¡œì„¸ìŠ¤ ì •ë³´
    airflow_processes = []
    for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):
        if 'airflow' in proc.info['name'].lower():
            airflow_processes.append(proc.info)
    
    # ë¦¬ì†ŒìŠ¤ ì •ë³´ ì €ì¥
    resource_info = {
        'timestamp': datetime.now().isoformat(),
        'cpu_percent': cpu_percent,
        'memory': {
            'total': memory.total,
            'available': memory.available,
            'percent': memory.percent
        },
        'disk': {
            'total': disk.total,
            'used': disk.used,
            'free': disk.free,
            'percent': (disk.used / disk.total) * 100
        },
        'airflow_processes': airflow_processes
    }
    
    # ë¦¬ì†ŒìŠ¤ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    with open('/tmp/resource_info.json', 'w') as f:
        json.dump(resource_info, f, indent=2)
    
    print("ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ:")
    print(f"CPU ì‚¬ìš©ë¥ : {cpu_percent}%")
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory.percent}%")
    print(f"ë””ìŠ¤í¬ ì‚¬ìš©ë¥ : {(disk.used / disk.total) * 100:.2f}%")
    
    return resource_info

resource_monitoring_task = PythonOperator(
    task_id='resource_monitoring',
    python_callable=resource_monitoring,
    dag=dag,
)
```

## ì£¼ì˜ì‚¬í•­ ë° ëª¨ë²” ì‚¬ë¡€

### 1. DAG ì„¤ê³„
- **ë‹¨ìˆœì„±**: DAGë¥¼ ë‹¨ìˆœí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ê³„
- **ì¬ì‚¬ìš©ì„±**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
- **ì˜ì¡´ì„±**: ëª…í™•í•œ ì‘ì—… ì˜ì¡´ì„± ì •ì˜
- **ì—ëŸ¬ ì²˜ë¦¬**: ì ì ˆí•œ ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„

### 2. ì„±ëŠ¥ ìµœì í™”
- **ë¦¬ì†ŒìŠ¤ ê´€ë¦¬**: ì ì ˆí•œ ë¦¬ì†ŒìŠ¤ í• ë‹¹
- **ë³‘ë ¬ ì²˜ë¦¬**: ê°€ëŠ¥í•œ ê²½ìš° ë³‘ë ¬ ì²˜ë¦¬ í™œìš©
- **ìºì‹±**: ì¤‘ë³µ ì‘ì—… ë°©ì§€ë¥¼ ìœ„í•œ ìºì‹±
- **ëª¨ë‹ˆí„°ë§**: ì§€ì†ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 3. ë³´ì•ˆ
- **ì ‘ê·¼ ì œì–´**: ì ì ˆí•œ ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬
- **ë¹„ë°€ ê´€ë¦¬**: ë¯¼ê°í•œ ì •ë³´ ë³´í˜¸
- **ê°ì‚¬**: ì‘ì—… ì‹¤í–‰ ë¡œê·¸ ë° ê°ì‚¬
- **ë°±ì—…**: ì •ê¸°ì ì¸ ë°±ì—… ìˆ˜í–‰

## ë§ˆë¬´ë¦¬

Apache AirflowëŠ” ë³µì¡í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•˜ê¸° ìœ„í•œ ê°•ë ¥í•œ í”Œë«í¼ì…ë‹ˆë‹¤. DAGë¥¼ í†µí•œ ì›Œí¬í”Œë¡œìš° ì •ì˜, ìŠ¤ì¼€ì¤„ë§, ëª¨ë‹ˆí„°ë§ ë“±ì˜ ê¸°ëŠ¥ì„ í†µí•´ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì ˆí•œ ì„¤ê³„ì™€ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ ì‹¤ë¬´ì—ì„œ ìš”êµ¬ë˜ëŠ” ë†’ì€ í’ˆì§ˆì˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ìš´ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
